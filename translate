简介：

图形数据库GraphDB是LinkedIn的实时分布式社交网络服务的存储层。我们的服务处理简单的查询(like the first and second degree networks of LinkedIn members as well),还有一些更复杂的查询，比如各个成员之间的图形距离，各个成员之间的路径等等。我们支持多节点和边缘类型以及process all queries  on the fly。

LinkedIn上的每一页都会在GraphDB中生成多个查询。这意味着GraphDB每秒可以处理成千上万条查询，而其中99%的响应有微秒级的延迟(一般是10微秒)。因此，来自GraphDB的即使是5毫秒的响应延迟也会使得Linkedln.com有明显的延迟比。

在整个2013年，我们在GraphDB的响应延迟中看到了间歇性的偶然的严重峰值。然后就深入调查这些峰值，并且我们的努力使得我们深入到了linux内核如何管理虚拟内存NUMA系统的细节。在一个nutshell中，linux对NUMA的优化有严重的副作用，这个副作用直接影响到了我们的延迟。
我们觉得linux中跑的任何在线的、低延迟的数据库系统都可一从我们这篇文章中获益:实施我们的优化以后，可以看到错误率(eg.延迟或者超时的请求)降到了原来的四分之一。

文档的第一部分提出了相关的背景信息：GrapgDB数据库如何管理它的数据，我们的问题症状，以及linux虚拟内存管理系统是如何工作的。文档的第二部分，会详细说明方法、观察法以及得出的结论，和在获取root权限时引起的问题。


正式开始
1）GraphDB如何管理自己的数据
GraphDB是一个很重要的内存数据库。对于读操作，我们会通过mmap将数据文件映射到内存空间，并将活跃的部分一直保持在内存中。通过对整个数据的设置我们的读操作是相当随机的，并且，99%的请求都有微秒级的延迟。一个典型的GraphDB主机有48GB的物理内存，使用了其中的20DB：15GB用于存放映射的数据，这些数据在堆外；还有5GB为JVM堆所用。

对于写操作，我们有一个日志结构的存储系统。我们将所有的数据分成10MB大小的只能以追加形式组成的段。目前，每个GraphDB主机有大约1500个活跃的片段，其中只有25个片段是在任何既定时刻都可写。其余的1475个片段是只读的。

作为一个日志结构的存储，我们会定期压缩(periodic compactions)我们的数据。而且，我们的压缩方案相当强的，每个主机每天大约产生900个数据片段。也就是说(that is)每天每台主机大约产生9GB大小有价值的数据文件，即使我们的数据覆盖区以那个数据的很小一部分增长。结果是，在一个48GB的主机上，我们在5天之内就可以用垃圾数据装满所有的页缓存。

2）问题症状
我们的性能问题的主要症状是GraphDB响应延迟中的峰值。这些峰值会伴随着一个非常高值的直接页扫描和低存储效率，这些都可以用sar看到。尤其，在sar -B所显示的'pgscand/s'列的输出对于虚拟内存效率为0%的几个小时里，可能会显示每秒1到5百万页的扫描量。

在几次性能下降中，系统可能处于并没有明显的内存压力的状态：可能会有大量的不活跃的缓存页会通过/proc/meminfo文件记录。而且，并不是pgscand/s中的每个峰值都会在GraphDB通信时引起一个延迟峰值。

我们可能会迷惑于两个问题：
1.如果系统没有明显的内存压力，为什么内核还要进行页扫描？
2.即使内核扫描页，为何还有响应延迟？只有写线程需要分配新的内存，而读和写线程是解耦合的。所以为什么一个会阻塞另一个呢？

这些问题的答案会引导我们探索linux中对NUMA系统的优化。事实上，会引导我们研究linux中的可回收区的特性。如果你不是很了解NUMA，linux和可回收区，不要担心，下一节就会讲。

3）一点关于linux NUMA以及可回收区的知识：
理解linux如何处理NUMA体系结构对于理解root给我们造成的问题至关重要。这里有些关于linux和NUMA的优秀资源：

(1)Jeff Frost: PostgreSQL, NUMA and zone reclaim mode on Linux. If you are pressed for time, this is the one you should read.
(2)Christoph Lameter: Non Uniform Memory Access, an overview. In particular read about how Linux reclaims memory from NUMA zones.
(3)Jeremy Cole: The MySQL "swap insanity" problem and the effects of NUMA architecture.

这对于理解linux中用于从页缓存回收页面的机制也很重要。
在一个nutshell中，linux对于每个NUMA区都维护着3组页：活跃列，不活跃列，以及空闲列。新页的分配会将空闲列中的页移到活跃列中。用LRU算法将活跃列中的页移动到不活跃的列。然后再从不活跃列移到空闲列。

再看完这些文档后，我们尝试着在生产主机中关掉可回收区模式。当这样做了后，我们的机器性能立即提升了。这就是为什么我们关注理解可回收区都做了些什么以及它是怎么样影响我们的性能。

本文剩下的部分是深入研究linux的可回收区行为。如果你不是很熟悉什么是可回收区，那么请至少(at the very least)阅读上面 Jeff Frost's article.

重述并理解linux的可回收区行为
1)做实验
为了明白是什么触发了一个可回收区，并且可回收区是如何影响我们的性能的，我们写一个程序来模拟GraphDB的读写行为。

将这个程序以24小时为周期运行。开始的17小时，程序跑的时候打开可回收设置。余下的7小时程序跑的时候关掉可回收设置。程序以24小时为周期不可中断地跑。环境的唯一变化就是在最初的17小时中通过向/proc/sys/vm/zone_reclaim_mode中写“0”来禁止区间可回收。

这里是该程序所做的工作：
1.映射了2500x10MB的数据文件，通过读取这些文件，然后解除映射来达到用垃圾数据填充linux页缓存的目的。这可能会使得系统进入到一种类似于一个GraphDB主机经过几天正常的运行时间后的状态。
2.一组读线程映射另外一组大小为2500x10MB的文件，并且，其中读部分是随机的。这些2500个文件是由活跃组组成的。这用来模拟GraphDB的读行为。
3.一组写线程不停的创建10MB的文件。一旦一个写线程填充了一个文件，它可能会从活跃列中随机选择一个文件，解除其映射，并且用一个新创建的文件来替换之。这用来模拟GraphDB的写行为。
4.最后，如果一个读线程花费超过100ms的时间来完成一个读操作，就可能会打印出usr,sys以及特殊访问的超时时间。这使得我们能够保持跟踪读性能的下降。

我们将这个程序放在一个物理内存为48GB大小的主机上运行。设置占用25GB的内存，并且在该主机上除了这个程序再无其他程序运行，所以我们的主机并没有任何实际的内存压力。

2）理解是什么触发了可回收区
当一个进程请求一个页时，内核会检查优先选择的NUMA区是否还有足够的内存空间，检查是否还有超过该空间总量的1%的可分配空间。



